---
layout: post
title: Convolutional Neural Networks (CNN)
tags: [supervised, machine-learning]
categories: notes
---
Like most things, Multilayer Perceptron (MLP) has its own set of limitations.
- Since it's **fully connected**, we'd have lots of parameters (weights), so can the computational complexity.
- It takes inputs as vector, so any 2D input, say an image grid, must be converted to a vector before MLP can take it up. However, often case the knowledge of where the pixels are located in reference to one another are "lost in translation". 
- Many training examples, 100 features not computationally feasible
This is where CNN (Convolutional) comes in to save the day because 
- Not all inputs are connected to every unit in the next layer.
- It takes in matrices as input.

softmax usually added to last fully connected to give probabilities of each output

### Epoch, Iteration, Batch Size
If there are ‘N’ records in the data set and let’s say we train our model in batches of ‘n’ records then
- One **epoch** is one pass (feedforward+backpropagation) till each of the N records is used to train the model
- One **iteration** = $$frac{N}{n}$$ assuming each record is only picked once.
If we assume random training, iterations = some number 'X’, where we take ‘n’ records from ‘N’ randomly ‘X’ number of times to train the model. In this case, a record may be used [0, X] times to train the model.
- Batch size

You have a training data which you shuffle and pick mini-batches from it. When you adjust your weights and biases using one mini-batch, you have completed one iteration. Once you run out of your mini-batches, you have completed an epoch. Then you shuffle your training data again, pick your mini-batches again, and iterate through all of them again. That would be your second epoch.

 the step is taken after each iteration, as Franck Dernoncourt's answer implied; that's what we do with the information from the backwards pass. In a mini-batch gradient descent with m iterations per epoch, we update the parameters m times per epoch. 

https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks

ConvNets only capture local "spatial" patterns in data. If the data can't be made to look like an image, ConvNets are less useful [(souce)](1)

To construct a CNN:
- Create a sliding window
- Blue matrix = convolutional layer
- Weights from nodes of the same window can be represented in a grid form for calculation convenience.

<p align="center">
  <img src="../../img/post-img/nn/cnn/2.png" height="80%" width="80%">
</p>

<p align="center">
  <img src="../../img/post-img/nn/cnn/3.png" height="80%" width="80%">
</p>

- Visualizing the learned filter will tell us the patterns the filter is designed to detect.
- Run process with many different filters

ReLU adds non-linearity to the network in a sense that each feature might not be a linear function with the predicted output

Input layer is comprised of input features


Filters that function as edge detector are very imp

3D images are seen as 3d array with the depth array seen as 3 arrays R, G, B
Filters are now 3D, a stack of 3 2D arrays

You would then get a convolutional layer, a stack of 2D feature(activation) maps, each of which comes from a filter.
<p align="center">
  <img src="../../img/post-img/nn/cnn/4.png" height="80%" width="80%">
</p>

This layer can then be viewed as the original grid, and filtered in the same manner.

By specifying a **loss function**, we can implicitly tell CNNs what patterns the filters should look for.

To increase number of nodes in Convolution Layer = Increase number of filters

### Stride and Padding

**Stride** is the number of pixels we're shifting every time we move a filter. This could result in the sliding filter going out of the image boundaries as shown below.
<p align="center">
  <img src="../../img/post-img/nn/cnn/5.png" height="60%" width="60%">
</p>
  - **Valid** padding is when you don't stride over the edge of the input.
  - **Same** padding maintains the same dimension for the output as the input as we pad 0 to the input edges.
  <p align="center">
    <img src="../../img/post-img/nn/cnn/10.png" height="60%" width="60%">
</p>

**Patch** = kernel





goal is to make deeper
<p align="center">
  <img src="../../img/post-img/nn/cnn/8.png" height="80%" width="80%">
</p>
<p align="center">
  <img src="../../img/post-img/nn/cnn/9.png" height="80%" width="80%">
</p>
--------------------------------------------------------------

The number of **parameters** in a convolutional layer (CL) is
> filterDim1 * filterDim2 * DepthOfPreviousLayer * NumFilters + NumFilters(Biases)

The **shape** of a CL is
* with padding:
\\[height = ceil(\frac{heightOfPreviousLayer}{Stride})\\]
\\[width = ceil(\frac{widthOfPreviousLayer}{Stride})\\]
* without padding:
\\[height = ceil(\frac{heightOfPreviousLayer - heightOfFilter + 1}{Stride})\\]
\\[width = ceil(\frac{widthOfPreviousLayer - widthOfFilter + 1}{Stride})\\]

The **depth** of a CL is always equal to NumOfFilters

## Improvements

1. **Pooling** because increasing the stride might be too aggressive and causing us to lose info
- **Max Pooling**: 
  - need stride and kernel_size to get the max in each section of feature map
  - often run at lower stride => more computationally expensive
  <p align="center">
  <img src="../../img/post-img/nn/cnn/13.png" height="60%" width="60%">
</p>

- **Global Average**: 
  - for each feature map, get the average
  - reduce stack of arrays down to a vector
- and many more
2. 1x1 Convolutions
cheap 
<p align="center">
  <img src="../../img/post-img/nn/cnn/11.png" height="20%" width="20%">
</p>
3. Inception
<p align="center">
  <img src="../../img/post-img/nn/cnn/12.png" height="20%" width="20%">
</p>
### CNN architecture
<p align="center">
  <img src="../../img/post-img/nn/cnn/7.png" height="80%" width="80%">
</p>
After alternating between convoluting and pooling, we will get a much deeper matrix in which each layer filters for one thing, which we can then flatten and feed into a deep NN.

### Data Augmentation
The idea is by introducing many variants in scale, rotation and translation(position) of the examples in our training set, we'd avoid overfitting even more. More in this [great blog post](http://machinelearningmastery.com/image-augmentation-deep-learning-keras/). 


### Transfer Learning
is the idea of using a pre-trained neural network, and replacing its last fully connected classification layers with a layer matching the number of classes in the new data set.
- In the case that the new data set is small, we would want to hold the pre-trained weights constant to avoid overfitting.
[Reference](https://classroom.udacity.com/nanodegrees/nd009/parts/99115afc-e849-48cf-a580-cb22eea2ba1b/modules/777db663-2b0d-4040-9ae4-bf8c6ab8f157/lessons/52fc79a7-13ff-4065-b3c6-8203ec9ef60c/concepts/8c202ff3-aab5-46c3-8ed1-0154fa7b566b)


[1]: https://www.youtube.com/watch?v=FmpDIaiMIeA