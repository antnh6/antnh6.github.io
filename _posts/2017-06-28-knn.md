---
layout: post
title: K-Nearest Neighbors (KNN)
tags: [supervised]
categories: notes
---

Up to this point, I have just used the training examples to come up with a function to predict unseen data, throw away those examples, then run with the function. 

However, **kNN** follows a kind of different paradigm in which it predicts each unseen data point by reasonably assuming that it will resemble its k "closest" training examples. 
* This means that all the training data is needed during the teting phase. 
* Which means that the training phase is gonna be really fast since it's not using the data to do any generalization. 
* Which means the testing phase is slow and space-costly. 
* The technical term for this type of algorithm is *lazy* :D [(Source)][2]

It now makes sense why this algorithm is also called **Instance-based Learning** because its parameters are actually the instances themselves. Technical term for this is **nonparametric**.

How we measure **closeness** is obviously important, especially when the features need not be quantitative, but can also be categorical. The techinical term for calculating this distance is [**similarity measurement**][3].

In sklearn, the hyper-parameter *weights* can be set to 'distance' for a **Weighted kNN** which follows this notion that a closer training example would have a greater influence than a farther one.

In order to pick the right value for k, I'd plot the training error and testing error against increasing values for k. Note k=1 means the closet point to any data point is itself, so training error = 0 in this case.

|Training Error| Testing Error |
|:---:|:---:|
|![img](../../img/post-img/supervised/knn/2.png) |![img](../../img/post-img/supervised/knn/1.png)  

From the graphs, it looks like k = 9 is about right.[ (Source)][1]

[1]: https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/
[2]:https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/
[3]: http://people.revoledu.com/kardi/tutorial/Similarity/index.html