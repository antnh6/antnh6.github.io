---
layout: post
title: Useful R commands/Tips
tags: [R]
categories: notes
---
* R is indexed 1.
* **?anything** to find more about anything
* **read.csv**(fileName) must nagivate to folder containing the file before calling the fn 
* **summary**()
    * also used to see the # of NA's
* **str**()
* **table**() 
    * when 2 args are used, first will be listed on the left, second on top
    * also used to create a confusion matrix
* **nrow**() to get the number oftraining examples
* **which.max**(column) to find the row number of the max value of that column
    * which(item) to find index of item
* **hist**()
* **boxplot**(oneFeature ~ anotherFeature)
* **sort**(data, decreasing = TRUE/FALSE, index.return = T/F and $ix to get indices from original data)
* Top2 = **subset**(mvt, LocationDescription=="ALLEY" \| LocationDescription=="GAS STATION")) OR
    * Top5 = subset(mvt, LocationDescription **%in%** c("STREET", "ALLEY"))
* **plot** (x,y) 
    * type="l" to get smooth graph
    * col = "red"
    * colors() to get all possible values for argument 'col'
    * lty=2 to get different type of line
* **lines**() to add line to existing graph
* **points** to add points
    * pch= parameter to get solid points 
* **abline**(v=as.Date(c(someDate)), lwd=someNum) to pinpoint on existing Graph
    * lwd is for line thickness
* **sample**(data, sizeOfSample) takes a random sample of sizeOfSample from data 
* **tapply**(1stArg, 2ndArg, function) sort by 2ndArg, then apply function on 1stArg
    * Ex:
    sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean, na.rm=TRUE)) will return proportion of interviewees from each metropolitan area who have not received a high school diploma, ignoring missing values
* **ctrl+L** to clear console
* **is.na**(column)
* **mean**(c(TRUE,TRUE,TRUE,FALSe)) gives 0.75
* Convert code to values by connecting 2 dataframes aka **mapping**
    *Ex: combined = merge(CPS,MetroAreaMap, by.x="MetroAreaCode",by.y="Code",all.x=TRUE). Last arg is to keep all rows from CPS as NAs even if there is no match.
* **jitter()** 
    * adds or subtracts a small amount of random noise to the values passed to it, and two runs will yield different results correct
* **lm**(thingToPredict ~ IndependentVar1 + IndependentVar2, data=source)
    * use **.** in place of the Independent Variables to take them all
        * to "quickly" deselect CATEGORICAL variables 
            nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
            SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
        * for Numeric variables, simply subtract 
            SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
    * **residuals** to get the difference vector between lm and actual
    * **predict**(modelName, testSet)
    * **step**(linearRegModel) is a function that automates the procedure of trying different combo of variables to find a good compromise of model simplicity (num of independent variables and the quality of the model: R<sup>2</sup>)
    * **glm** to build logistic regression
        * predict(type="response" to get probabilities, newdata=dataSet)

* **cor**(variable1, variable) to calculate the correlation between two variables
    * cor(dataset) to see correlation between all pairs of variables in the dataset (might need to do subset to leave out nonnumeric values)
* **na.omit**(dataset)
* **matrix**(c(1,2,3,4), byrow=TRUE,nrow=2) to create a 2x2 matrix, filled by row
* **sign**(value) returns 1 when value is positive, -1 when negative, and 0 when tie 
* **factor** --- the same as OneHotEncoding in Python
    * choose most popular factor to be the reference, then create a feature for each of the other factors
    * As an example, consider the unordered factor variable "color", with levels "red", "green", and "blue". If "green" were the reference level, then we would add binary variables "colorred" and "colorblue" to a linear regression problem. All red examples would have colorred=1 and colorblue=0. All blue examples would have colorred=0 and colorblue=1. All green examples would have colorred=0 and colorblue=0.
    * R automatically sets the reference level to be the first alphabetically => to reset level use **reset**(data$variable, factorToBeTheReference)
* **set.seed**(num) to get reproducible random numbers
* package **zoo** is used to manipulate time series data
    *Ex: lag(zoo(FluTrain$ILI), -2, na.pad=TRUE) creates a 2 week lag for every observation, which means to predict current week's value, use the observation from 2 weeks ago ; na.pad = TRUE adds missing values for the first two weeks of our dataset, where we can't compute the data from 2 weeks earlier.
* package **caTools** to split data into training and testing sets
    * sample.split(DependentVar, SplitRatio=percentageOfDataInTrainingSet)
        train = subset(data, split == TRUE)
          test = subset(data, split == FALSE)
* package **ROCR** to create a ROCR
    * prediction(predicted, actual)
    * performace(prediction_object, measure="auc" to get the auc value or measure = "tpr", x.measure = "fpr") to get the plot)
    * plot (performance)
    * as.numeric(perf@y.values)
    * plot(colorize = T/F, print.cutoffs.at=seq, text.adj)
* package **mice** to use Multiple Imputation by Chained Equations
    * complete(mice(dataSet)) to fill in missing values differently for each run
* package **rpart** and **rpart.plot** to build decision tree
    * model = rpart(Dependent ~ Independent, data= , method="class" for classification, minbucket=/cp=)
    * adds parms=list(loss=PenaltyMatrix) with self-defined Penalty Matrix to tell the algorithm to consider it when training the model
    * prp(model, digits=6 to get trailing digits) to plot tree
    * predict(model, newdata= , type = "class" to get majority class, leave out type to get probability as in log reg)
* package **randomForest**
    * nodesize = min number of observations in a subset
    * ntree = number of trees in a forest
    * varUsed(model, count=TRUE) to get the counts for each feature when it is selected for splitting
    * varImpPlot(model) to get the impurity measure for each feature
* package **caret** and **e1071** to perform cross validation using kfolds and a sequence of parameter values
    * Ex: cv with different choices of parameter cp for CART model
        numFolds = trainControl( method = "cv", number = 10 )
        cpGrid = expand.grid( .cp = seq(0.01,0.5,0.01)) 
        train(Reverse ~ Circuit + Issue, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
* **as.factor**(variable) to convert variable to a factor variable, i.e. instead of viewing each instance as a value, view it as a category sort of
