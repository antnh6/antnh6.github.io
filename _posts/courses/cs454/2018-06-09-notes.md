---
layout: post
title: CS454 
subtitle: Distributed Systems
image: /img/avatar-icon.png
categories: notes
tags: [template, markdown]
---

The result of the advent of powerful and networked machines is that it is now not only feasible, but alsoeasy, to put together a computing system composed of a large numbers of networked computers, be they large or small. These computers are generally **geographically dispersed**, for which reason they are usually said to form a **distributed system**.

A distributed system is a collection of autonomous computing elements (**node**, can be hardware device or software process) that appears to its users (be they people or applications) as a single coherent system.

Each node will have its own notion of time, i.e. it's not always safe to assume that there is something like a global clock.

Managing groups of nodes can be tricky. First, we need a way to authenticate nodes that will not easily create a scalability bottleneck down the road and another way to ascertainif they are indeed talking to nodes within their group. 

Two types of overlay networks:

- Structured: each nodes has a well-defined set of neighbors
- Unstructured: ea node is randomly connected to other nodes.

**Distribution Transparency** tries to make the distribution
of processes and resources transparent, that is, invisible, to end users and
applications. Types:

- access
- location
- relocation
- migration
- replication
- concurrency
- failure

Leslie Lamport

In a sense, middleware is the same to a distributed system as what an
operating system is to a computer: a manager of resources offering its ap-
plications to efficiently share and deploy those resources across a network.

## Design Goals

A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable.

Resource sharing in distributed systems is perhaps best illustrated by the success of file-sharing peer-to-peer networks like BitTorrent.

The user cannot tell whether the server is actually down or that the network is badly congested.

Transparency only works to a certain degree. 

Signal transmission is not only limited by the speed of light, but also by limited processing capacities and delays in the intermediate switches.

The conclusion is that aiming for distribution transparency may be a
nice goal when designing and implementing distributed systems, but that
it should be considered together with other issues such as performance
and comprehensibility.

**Scalability dimensions**

- Size scalability
- Geographical scalability: negligible latency between LAN components but considerable in WAN. 2 more pts.
- Administrative scalability: many components of a ds that reside within a single domain can often be trusted by users operating within that same domain, but not necessarily for other domains.

Filling in forms can be done by sending a separate message for each field and
waiting for an acknowledgment from the server, as shown in Figure 1.4(a). For
example, the server may check for syntactic errors before accepting an entry.
A much better solution is to ship the code for filling in the form, and possibly
checking the entries, to the client, and have the client return a completed
form, as shown in Figure 1.4(b). This approach of shipping code is widely
supported by the Web by means of Java applets and Javascript.

Fixes: 

- Hiding communication latencies: anything that can be done on the client side should be done on that side.
- Partitioning and distributing: e.g. The naming service, as provided by DNS, is distributed across several machines in a tree form, thus avoiding that a single server has to deal with all requests for name resolution.
- Replication of components
    - One serious problem with **caching**(a decision made by the client of a resource and not by the owner of a resource) is **consistency**. An update must be immediately propagated to all other copies.

## Types of DS

- **Distributed shared-memory multicomputers** (DSM) allows a processor to address a memory location at another computer as if it were local memory by mapping all main-memory pages of the various processors into a single virtual address space. SLOW in practice.
- **Cluster Computing** is used for parallel programming in which a single program is run in parallel on multiple machines.

<p align="center">
  <img src="../../img/post-img/cs454/1.png" height="80%" width="80%">
</p>

- **Grid Computing** resources from different organizations are brought together to allow the collaboration of a group of people from different institutions, indeed forming a federation of systems. Such a collaboration is realized in the form of a **virtual organization**.
- **Cloud Computing** = utility computing = be charged per-resource basis

(Last Updated: June 9, 2018)