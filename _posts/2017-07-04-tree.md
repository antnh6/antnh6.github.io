---
layout: post
title: Decision Tree (CART)
tags: [supervised, machine-learning]
categories: notes
---

# Decision Tree
CART == Classification and Regression Tree

A CART model is **non-linear** unlike logistic or regression, i.e. there is no equation to express relationship between independent and dependent variables.

It can be used to predict both categorical and continuous (not so common) outcomes.

**ID3 Algorithm**
- Pick the "best" feature for root node
- For each of value of the feature, create new descendant of root node
- Sort training examples according to their values to the descendants
- If training examples are classified perfectly, then STOP, else, repeat step 1 over new leaf nodes

### How to pick the best feature
**Entropy**, denoted H(S) measures the impurity of a sample S. 
<p align="center">
    <img src="../../img/post-img/supervised/tree/7.png" height="30%" width="30%">
</p>
Entropy = 0 means that the sample is homogenous, while Entropy = 1 means that all classes are equally represented.
\begin{equation}
H(X) = - \sum_{i=1}^{n} P(X=i)\log_2P(X=i)
\end{equation}
where n is the number of possible values for X

One of the most common metrics used in determine the best feature to split on for the next node is **Information Gain** (= the expected reduction in entropy of target variable Y for data sample S, due to sorting on feature A). 
\begin{equation}
Gain(X,A) = H_{S}(Y) - H_{S}(Y|A)
\end{equation}
In the example below, we would choose Humidity over Wind because Gain(S,Humidity) is higher.
<p align="center">
    <img src="../../img/post-img/supervised/tree/4.png" height="80%" width="80%">
</p>

One important thing to know is that there might be many different trees that fit the same training set. Some are simpler, i.e. shorter, and the ID3 algorithm will favor simplicity, i.e. it will return the smallest acceptable tree. Why? One controversial reasoning is from **Occam's razor** which says out of all the hypotheses that fit the data, we will go with the simplest one because a shorter hypothesis is less likely to be a statistical coincidence. 

### Overfitting 
Overfitting happens when the true error rate over all data is higher than the error rate over the training data.
Two approaches in avoiding overfitting is:
- Stop growing when data split is not statistically significant
<p align="center">
    <img src="../../img/post-img/supervised/tree/6.png" height="60%" width="60%">
</p>
- Grow full tree, then post-prune
    - Do until further pruning is harmful:
        1. Evaluate impact of pruning each possible node and those below it on CV set
        2. Greedily remove the one that most improves CV accuracy

In R, parameter **cp** (complexity parameter) is similar to adjusted R<sup>2</sup> in LinReg and AIC in LogReg in that it measures the tradeoff between model complexity and accuracy. Each new split in the tree improves in sample error. We only split if the reduction in error is at least cp (in percentage).
If there was no such restriction (cp too low), we would keep splitting and our trees would be super complex - probably overfitted. If cp was too high, you would never split - underfitting => penalizing too many splits, i.e. find a tree that minimizes this sum below

### Penalty matrix
For multiclass classification problems, there is also this concept of penalty matrix where we penalty the algorithm more severely when it gets the more "important" classes wrong. 
=> aside from calculating the accuracy, we also take into consideration the penalty cost when evaluating different parameters. 

 ![penaltymatrix](../../img/post-img/supervised/tree/1.png){:height="60%" width="60%"}


# Random Forests
* The idea is to train many trees on "bagged"/"boostrapped" samples  of the data (chosen with replacement then choose the final result according to the majority of the votes from each tree.
<br>
* As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important. However, we can still compute metrics that give us insight into which variables are important.
    * One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split.
    * A different metric we can look at is related to "impurity" (Gini in R), which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest. A low Gini (i.e. higher descrease in Gini) means that a particular predictor variable plays a greater role in partitioning the data into the defined classes.
* Random forests tends to improve on CART in terms of predictive accuracy. 