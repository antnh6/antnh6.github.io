---
layout: post
title: PCA
tags: [data-analysis, unsupervised, preprocessing, data-transformation]
categories: notes
--- 
PCA (Principal Component Analysis) is a classic technique to reduce the number of dimensions we need to consider in a dataset (**dimensionality reduction**) and to reveal the **latent features** driving the patterns in our data. 
<!--<br>-->

It does so by finding the directions of maximum variance in our original high-dimensional data by minimizing the information loss when we project our data points onto these directions. This entails that PCA will transform our n-dimensional dataset into a *new* k-dimensional dataset [(source)][2]. These directions are called *principal components* and are actually *eigenvectors*, so they are orthogonal to one another. They can be also ranked in order of having the most variance to least. 

## Dimensionality Reduction
is useful in that it helps us:
* visualize our high-dimensional data in 2D/3D.
* reduce noise in our data (strongest principal components capturing the actual patterns in the data while smaller PCs representing noisy variations about those patterns).
* make other algorithms work better because fewer inputs (eigenfaces).

## Maximal Variance and Information Loss

* **Information Loss** is defined as the sum of all the projection lengths of our data onto a principle component, so by projecting each point onto the component with **maximal variance**, we will minimize the info loss the most. 

(It looks so in the diagram below, but I'm gonna have to read up on the math side of it).

<p align="center">
  <img src="../../img/post-img/unsupervised/pca/2.png" height="80%" width="80%">
</p>

## How many PCs to use?
No fast track here. Best way to go is to train on different number of PCs, and see how accuracy responds -- cut off when it becomes apparent that adding more PCs is overfitting our data.

## Limitations of PCA
* **Maximizing Spread**. The main assumption of PCA is that dimensions that reveal the largest spread among data points are the most useful. However, this may not be true. A popular counter example is the task of counting pancakes arranged in a stack when a stack is thin.
* **Orthogonal Components**. One major drawback of PCA is that the principal components it generates must be orthogonal. However, this assumption is restrictive as informative dimensions may not necessarily be orthogonal to each other.
* **Interpreting Components** Interpretations of generated components have to be inferred, and sometimes we may struggle to explain the combination of variables in a principal component.
[(source)][1]

## PCA is not Linear Regression
* On the left, we have the difference between the actual value and the predicted value (linReg). 
* On the right, it's the length of the projection (information loss). 
<p align="center">
  <img src="../../img/post-img/unsupervised/pca/1.png" height="80%" width="80%">
</p>


[1]: https://www.quora.com/What-is-an-intuitive-explanation-for-PCA/answer/Annalyn-Ng?srid=ugxJO
[2]: http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html