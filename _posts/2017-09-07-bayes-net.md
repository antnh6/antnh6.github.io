---
layout: post
title: Bayes Nets
tags: [AI]
categories: notes
---

- [Bayes Net](#bayes-net)
- [Conditional Independence](#conditional-independence)
    - [D-separation](#d-separation)
- [Probabilistic Inference](#probabilistic-inference)
    - Enumeration(#inference-by-enumeration)
    - Variable Elimination
    - Sampling
- [Learning Bayes Nets from Data]    

- Marginalization = P(oneVar)
- Conditional P( oneVar \| anotherVar)
- Joint Distribution P(Var1, Var2, ..., VarN)
- Joint Distribution Table may blow up very quickly if the domain for each variable is big, so by leveraging conditional indepences, we use cut down on the representation

# Representation
- can be causal or not
- must be directed, acyclic graphs

Bayes' nets = Belief networks = more like graphical models to compactly encode joint distributions
- has inside it conditional independence assumptions
A full joint distribution is huge in memory, so we use Bayes Net to represent. An N-node net where every node has up to k parents is O(N * $$2^{k+1}). We've gone down from $$2^{N}$$ so its important to keep k small.

Hidden variables are those that link queries and evidences

# Conditional Independence
A is conditional independent of B given C denoted $$ A ??? B | C $$
Unconditional Independence is rare, so Conditional Independence is our best bet in order to "get rid of" variables for a more compact representation
Ex: 
If causual X -> Y -> Z then it means Z can be seen as conditional independent of X given Y 
**Sum rule**
\begin{equation}
\begin{split}
\forall{x,y,z}: P(x,y|z) = P(x|z) \times P(y|z) 
\\
\forall{x,y,z}: P(x|y,z) = P(x|z)
\end{split}
\end{equation}
**CPT**(Conditional Independence tables) give us the Pr($$X_{i}$$ | Parent($$X_{i}$$)) for each $$X_{i}$$
$$
P(Query|Evidence) proportional \sum_{all other variables, not evidence} P(Query, Evidence, those variables)
$$

### D-separation
Given some evidence that two variables are independent, D-separation is the algorithm that tells if two variables are independent in a given BN by checking all undirected paths betwee those variables, if at least one is active then independence is not guaranteed.
Any Bayes Nets can be broken into triples that take the form of either:
- Causal Chain 
- Common Cause
- Common Effect - two causes of one effect (v-structure) is backwards from the other cases, where X and Y start out independent but get coupled when an effect is observe because it's more likely that either X or Y occured but not both. 
<p align="center">
    <img src="../../img/post-img/bayes-net/1.png" height="80%" width="80%">
</p>
Example 1: Single path - Conditional Independence?
<p align="center">
    <img src="../../img/post-img/bayes-net/2.png" height="50%" width="50%">
</p>
Example 2: Multiple paths
<p align="center">
    <img src="../../img/post-img/bayes-net/3.png" height="50%" width="50%">
</p>

# Probabilistic Inference
### Inference by Enumeration


arrows may encode causality, but really encodes assumption of conditional independence i.e.
P(x | all ordered nodes before it) = P(x |parent(x))

compute full joint (can be obtained from any conditionals) => inference
 <p align="center">
    <img src="../../img/post-img/bayes-net/8.png" height="50%" width="50%">
</p>

- Naive = by Enumeration is slow because we have to expand in order of number of hidden variables (which can get exponential)
### Variable Elimination
 still NP-hard, but usually much faster 
 <p align="center">
    <img src="../../img/post-img/bayes-net/9.png" height="50%" width="50%">
</p>
- Break full joint table into factors which are conditionals for oneVar given anotherVar where these variables can be instantiated or not.
- Operation 1: Join factors to full joint e.g. P(T) and P(R\|T) to P(R,T)
- Operation 2: Marginalization (shrink to one certain variable) e.g. P(R,T,L) to P(T,L) to P(L)
<p align="center">
    <img src="../../img/post-img/bayes-net/4.png" height="50%" width="50%">
</p>
IbE do all join on hidden variables first, then eliminate while VE join and elimnate on one hidden variable to form a new factor which can be joined and eliminated on another hidden variable.
VE in action
<p align="center">
    <img src="../../img/post-img/bayes-net/5.png" height="50%" width="50%">
</p>
We can be clever in the order in which we pick the hidden variables to minimize the size of the factor generated at each step (ones with more children picked last as they are present in more factors). Even so, the order can only help so much, in some problems, it is computationally expensive anyway.
<p align="center">
    <img src="../../img/post-img/bayes-net/6.png" height="50%" width="50%">
</p>
normalization
<p align="center">
    <img src="../../img/post-img/bayes-net/7.png" height="50%" width="50%">
</p>

### Sampling 
- **Prior Sampling** for all nodes x in Bayes net, sample Node from P(Node \| Parent(Node)) in the limit, it'll work out
- **Rejection Sampling** a slightly improved version of PS that short-circuits the sampling process before having to run through the entire net, i.e. if parent(Node) is not what we are looking for, then cut short
<p align="center">
    <img src="../../img/post-img/bayes-net/10.png" height="50%" width="50%">
</p>

- **Likelihood Weighting** 
maybe to scale each sample on how likely it happens
<p align="center">
    <img src="../../img/post-img/bayes-net/14.png" height="50%" width="50%">
</p>
<p align="center">
    <img src="../../img/post-img/bayes-net/13.png" height="50%" width="50%">
</p>
- **Gibbs Sampling** is a way to pick out samples in a smart way
    - Fix evidence
    - Instantiate all other variables randomly
    - Pick one variable that's not the evidence, resample it conditioned on the rest of variables. This is easier to do.
<p align="center">
    <img src="../../img/post-img/bayes-net/12.png" height="50%" width="50%">
</p>
    - Repeat whole process to get samples
<p align="center">
    <img src="../../img/post-img/bayes-net/11.png" height="50%" width="50%">
</p>

