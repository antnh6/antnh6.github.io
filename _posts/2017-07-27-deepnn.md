---
layout: post
title: Deep Neural Networks
tags: [deep-learning, machine-learning]
categories: notes
---
## Perceptron
There are two ways to represent a perceptron. One is
<p align="center">
    <img src="../../img/post-img/nn/deep/1.png" height="60%" width="60%">
</p>
or with the **bias** term as a fixed node

<p align="center">
    <img src="../../img/post-img/nn/deep/2.png" height="60%" width="60%">
</p>

### Activation function 
This function takes in a linear combination of the weights and the values as its input, and outputs discrete or continuous values depending on what type of activation function it is. 
<p align="center">
    <img src="../../img/post-img/nn/deep/5.png" height="60%" width="60%">
</p>
> NOTE: the Sigmoid function is a special case of the Logistic function 
<p align="center">
    <img src="../../img/post-img/nn/deep/16.png" height="70%" width="70%">
</p>

### Perceptron as Logical Operators
Perceptrons can be used to represent Logical Operators. Below is the multi-layer perceptron that calculates XOR. It contains 3 perceptrons of AND (A), OR (C), and NOT (B).
<p align="center">
    <img src="../../img/post-img/nn/deep/3.png" height="60%" width="60%">
</p>

### Error/Cost function
An **error function**, also called **cost function** quantifies the performance of our model under some set of parameters. For example, Misclassification error, Mean Squared error (MSE), and Average Cross Entropy error (ACE). ACE is usually preferred in DL because it converges faster during the backpropagation stage as the gradient does not get smaller as the predictions get closer to the actual outputs [(source)](1).
\begin{equation}
error = -\frac{1}{m} \sum_{i=1}^{m}\sum_{j=1}^{n}y_{ij}\times ln(\hat{y}_{ij})
\end{equation}
- **Cross Entropy** = -ln( Maxmimum Likelihood)
    - Since probabilities are all less than 1, taking **negative** ln will yield a positive sum which looks ... nicer.
    - maximizing Maximum Likelihood = minimizing Cross Entropy because 
\begin{equation}
-ln(biggerMaximumLikelihood) \rightarrow smallerCrossEntropy
\end{equation}

<p align="center">
    <img src="../../img/post-img/nn/deep/6.png" height="70%" width="70%">
</p>


### Multi-layer Perceptron
So far, we have just assumed that our data is linearly separable, but nothing is that easy IRL. By combining many Perceptrons, we can build a **Multi-Layer Perceptron** (MLP) that can separate our non-linear data by taking the outputs from one perceptron to be the inputs for another. 
<p align="center">
    <img src="../../img/post-img/nn/deep/8.png" height="80%" width="80%">
</p> 
Just like in the case for a simple perceptron above, there is nothing keeping us from adjusting the weight to "give more say" to the more important output from the previous layer
<p align="center">
    <img src="../../img/post-img/nn/deep/10.png" height="60%" width="60%">
</p> 

### Deep Neural Networks
are MLPs with more than one hidden layer - the technical term for any layer that is not the first (input) layer or the last(output) layer.
When the input layer is made up of 3 nodes, it means our data is in 3D, so the boundary "line" becomes the boundary plane. Deductively thinking, a n-dimensional dataset can be separated by a n-dimensional hyperplane.
<p align="center">
    <img src="../../img/post-img/nn/deep/11.png" height="40%" width="60%">
</p> 
- **Backpropagation** is a training algorithm used to update each weight in a way that causes the predicted outputs $$\hat{y}$$ to be closer the actual outputs *y*, thus minimizing the error for each output and the network as a whole. It consists of 2 steps: 
    - **Feedforward** - another algorithm that calculates the predicted outputs by feeding the outputs from one layer as inputs to the next layer to the next in a forward manner
    - **propagate** info gained from the error (roughly speaking, the sum of differences between predicted and actual outputs) **back** to the previous layers by adjusting the weights
- Applying an optimization algorithm such as **Gradient Descent $$\nabla{E}$$**  on the error function will tell us exactly how to tweak these weights *W*.
    <p align="center">
        <img src="../../img/post-img/nn/deep/12.png" height="80%" width="80%">
    </p> 
    - The **learning rate** $$\alpha$$ is how quickly a network abandons old beliefs for new ones because $$\alpha$$ determines how quickly or slowly we want to update the weights. In general, we want to find a learning rate (by trial and error) that is low enough that the network converges to something useful, but high enough that we don't have to spend years training it [(source)](2). In practice, people also often use decreasing $$\alpha$$ that is high when the error is high, and decreasing as the error gets smaller.
    <p align="center">
        <img src="../../img/post-img/nn/deep/15.png" height="70%" width="70%">
    </p> 
    - Gradient Descent Variants 
        - Batch GD 
            - is deterministic which means we will get the same optimum in the same number of iterations.
            - might take a long time if the training set is large, because at each iteration it needs to sum over all the training examples (hence the name batch) in order to update each of the weights
        - Stochastic GD
            - is ... stochastic which means that we are not guaranteed the same optimum in the same number of iterations
            - will only look at one random training example to update each of the weights
            - Due to its stochastic nature, the path towards the global cost minimum is not "direct" as in GD, but may go "zig-zag" if we are visualizing the cost surface in a 2D space [(source)][3].
        - Mini-batch GD
            - is a compromise between batch and stochastic GD where we sum over a random subset of the training examples every time
            <p align="center">
                <img src="../../img/post-img/nn/deep/18.png" height="70%" width="70%">
            </p> 
    - More on GD Optimizers [here](http://ruder.io/optimizing-gradient-descent/index.html)

### The Math
Here is the **gist** from this excellent [blog post](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/).
- Derivative of Error function wrt the weights from each layer
- Chain Rule
<p align="center">
    <img src="../../img/post-img/nn/deep/17.png" height="70%" width="70%">
</p> 


### Multi-Class Classification
- use the **Softmax** function to get the probability of being assigned to each class for each observation
\begin{equation}
output = \frac{e^{z^{i}}} {\sum_{j=0}^{k} e_{k}^{z^{i}}}
\end{equation}
where each $$z^{i}$$ is a value obtained from an activation function.

**Dropout** is a regularization technique used to prevent overfitting by "turning off" nodes with small probability during the training process.

**Random restart** is used to increase the probability of our model converging to the global minimum.

### References
These are the sources I used to make this post.
- [this Quora answer](https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network-How-does-this-function-in-a-human-neural-network-system/answer/Sebastian-Raschka-1?srid=ugxJO)
- [Udacity - Deep NN](https://classroom.udacity.com/nanodegrees/nd009/parts/99115afc-e849-48cf-a580-cb22eea2ba1b/modules/777db663-2b0d-4040-9ae4-bf8c6ab8f157/lessons/21d39927-8e4f-44ba-8425-d15889ac19e2/concepts/452bc8f6-304d-49d8-ba6d-b8a2e5ac9aaf)

[1]: https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
[2]: https://www.quora.com/What-is-the-learning-rate-in-neural-networks/answer/Conner-Davis-2?srid=ugxJO
[3]: https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent/answer/Sebastian-Raschka-1?srid=ugxJO