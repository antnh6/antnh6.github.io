---
layout: post
title: PCA
tags: [data-analysis, unsupervised, preprocessing, data-transformation]
categories: notes
--- 
PCA (Principal Component Analysis) is a classic technique to reduce the number of dimensions we need to consider in a dataset, say we have n features, now we only want k where k <= n. It does so by finding the directions of maximum variance in our original high-dimensional data by minimizing the information loss when we project our data points onto these directions. This entails that we will get a new *transformed* set of k-dimensional data onto which we can now train any learner instead of our n-dim set [(source)][2]. These directions are called principal components and are actually eigenvectors, so they are orthogonal to one another. They can be also ranked wrt having the most variance to least. 

Limitations [(source)][1]: 
* **Maximizing Spread**. The main assumption of PCA is that dimensions that reveal the largest spread among data points are the most useful. However, this may not be true. A popular counter example is the task of counting pancakes arranged in a stack when a stack is thin.
* **Orthogonal Components**. One major drawback of PCA is that the principal components it generates must be orthogonal. However, this assumption is restrictive as informative dimensions may not necessarily be orthogonal to each other.
* **Interpreting Components** Interpretations of generated components have to be inferred, and sometimes we may struggle to explain the combination of variables in a principal component.

## PCA is not Linear Regression
On the left, we have the difference between the actual value and the predicted value. On the right, it's just the length of the projection. 
<p align="center">
  <img src="../../img/post-img/unsupervised/pca/1.png" height="80%" width="80%">
</p>

## Maximal Variance and Information Loss

* **Information Loss** is defined as the length of the projection of a data point onto a principle component, so by projecting each point onto the component with maximal variance, we will minimize the info loss the most.

<p align="center">
  <img src="../../img/post-img/unsupervised/pca/2.png" height="80%" width="80%">
</p>

[1]: https://www.quora.com/What-is-an-intuitive-explanation-for-PCA/answer/Annalyn-Ng?srid=ugxJO
[2]: http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html