---
layout: post
title: Decision Tree (CART)
tags: [supervised, machine-learning]
categories: notes
---
CART == Classification and Regression Tree

A CART model is non-linear unlike logistic or regression, i.e. there is no equation to express relationship between independent and dependent variables.

When comparing CART and logistic regression, CART often performs a little worse than logistic regression in out-of-sample accuracy. However, the CART model is often much simpler to describe and understand.

### K-fold Cross Validation
to select the best parameter (minbucket/cp in this case) value for training a CART model. 

Parameter **cp** (complexity parameter) is similar to adjusted R<sup>2</sup> in LinReg and AIC in LogReg in that it measures the tradeoff between model complexity and accuracy. Each new split in the tree improves in sample error. We only split if the reduction in error is at least cp (in percentage).
If there was no such restriction (cp too low), we would keep splitting and our trees would be super complex - probably overfitted. If cp was too high, you would never split - underfitting => penalizing too many splits, i.e. find a tree that minimizes this sum below

![RSS+splitpenalty](../../img/post-img/supervised/tree/2.png){:height="30%" width="30%" align = center}

![cp](../../img/post-img/supervised/tree/3.png){:height="30%" width="30%" align = center}


[img]
[img]

You might be wondering why we used cross-validation on our CART model, but not on our random forest model. According to the creaters of the random forest algorithm, the model is not very sensitive to the parameters and therefore does not easily overfit to the training set.

### Penalty matrix
For multiclass classification problems, there is also this concept of penalty matrix where we penalty the algorithm more severely when it gets the more "important" classes wrong. 
=> aside from calculating the accuracy, we also take into consideration the penalty cost when evaluating different parameters. 

 ![penaltymatrix](../../img/post-img/supervised/tree/1.png){:height="60%" width="60%"}


## Random Forests
* The idea is to train many trees on "bagged"/"boostrapped" samples  of the data (chosen with replacement then choose the final result according to the majority of the votes from each tree.
<br>
* As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important. However, we can still compute metrics that give us insight into which variables are important.
    * One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split.
    * A different metric we can look at is related to "impurity" (Gini in R), which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest. A low Gini (i.e. higher descrease in Gini) means that a particular predictor variable plays a greater role in partitioning the data into the defined classes.
* Random forests tends to improve on CART in terms of predictive accuracy. 