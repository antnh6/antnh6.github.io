---
layout: post
title: Useful R commands/Tips
tags: [R]
categories: notes
---
* R is indexed 1.
* **?anything** to find more about anything
* **read.csv**(fileName) must nagivate to folder containing the file before calling the fn
    * use parameter stringsAsFactors = FALSE when doing text analytics
    * read.table(txt extension)
    * header = FALSE for files with no headers
    * sep 
* **summary**()
    * also used to see the # of NA's
* **str**()
* **table**() 
    * when 2 args are used, first will be listed on the left, second on top
    * also used to create a confusion matrix
* **nrow**() to get the number oftraining examples
* **which.max**(column) to find the row number of the max value of that column
    * which(item) to find index of item
* **hist**()
* **boxplot**(oneFeature ~ anotherFeature)
* **sort**(data, decreasing = TRUE/FALSE, index.return = T/F and $ix to get indices from original data)
* Top2 = **subset**(mvt, LocationDescription=="ALLEY" \| LocationDescription=="GAS STATION")) OR
    * Top5 = subset(mvt, LocationDescription **%in%** c("STREET", "ALLEY"))
* **plot** (x,y) 
    * type="l" to get smooth graph
    * col = "red"
    * colors() to get all possible values for argument 'col'
    * lty=2 to get different type of line
* **lines**() to add line to existing graph
* **points** to add points
    * pch= parameter to get solid points 
* **abline**(v=as.Date(c(someDate)), lwd=someNum) to pinpoint on existing Graph
    * lwd is for line thickness
* **sample**(data, sizeOfSample) takes a random sample of sizeOfSample from data 
* **tapply**(1stArg, 2ndArg, function) sort by 2ndArg, then apply function on 1stArg
    * Ex:
    sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean, na.rm=TRUE)) will return proportion of interviewees from each metropolitan area who have not received a high school diploma, ignoring missing values
* **ctrl+L** to clear console
* **is.na**(column)
* **mean**(c(TRUE,TRUE,TRUE,FALSe)) gives 0.75
* Convert code to values by connecting 2 dataframes aka **mapping**
    *Ex: combined = merge(CPS,MetroAreaMap, by.x="MetroAreaCode",by.y="Code",all.x=TRUE). Last arg is to keep all rows from CPS as NAs even if there is no match.
* **jitter()** 
    * adds or subtracts a small amount of random noise to the values passed to it, and two runs will yield different results correct
* **lm**(thingToPredict ~ IndependentVar1 + IndependentVar2, data=source)
    * use **.** in place of the Independent Variables to take them all
        * to "quickly" deselect CATEGORICAL variables 
            nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
            SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
        * for Numeric variables, simply subtract 
            SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
    * **residuals** to get the difference vector between lm and actual
    * **predict**(modelName, testSet)
    * **step**(linearRegModel) is a function that automates the procedure of trying different combo of variables to find a good compromise of model simplicity (num of independent variables and the quality of the model: R<sup>2</sup>)
    * **glm** to build logistic regression
        * predict(type="response" to get probabilities, newdata=dataSet)
* **cor**(variable1, variable) to calculate the correlation between two variables
    * cor(dataset) to see correlation between all pairs of variables in the dataset (might need to do subset to leave out nonnumeric values)
* **dist**(data, method = "euclidean") to get euclidean distance for hierachial clustering
* **hclust**(distances, method="ward.D2" to minimize variance within clusters and distance among clusters
    * plot() to get cluster dendrogram
    * rect.hclust(clusters, k = num, border = color) to visualize clusters on dendrogram
    * cutree(clusters, k = numClusters) to assign each data pt to cluster
    * split(data, clusterGroups) followed by lapply(spl, colMeans) to output centroids
* **image**(matrix, axes=T/F, col = grey(seq(0,1,length=256)) OR rainbow(k)) 
* **kmeans**(vector, centers=NumClusters, iter.max = 1000 to specify numIters)
    * str(kmeans) to see centroids
    * kmeans$withinss to get within-cluster sum of squares
    * use scree plot to determine optimal NumClusters by using sapply(2:10, function(x) sum(kmeans(healthyVector, centers=x, iter.max=1000)$withinss))
    * table(hclust,kmeans) to compare two methods
* package **flexclust** to fit kMeans Clutering onto test set
    * as.kcca(km, trainedModel) then use predict
* **dim**(matrix) = c(numRow, numCol) to tweak matrix into one of specified size
* **na.omit**(dataset)
* **nchar**(txt) count num char in text
* **strwrap**(longString) to display longString in multiple lines
* **matrix**(c(1,2,3,4), byrow=TRUE,nrow=2) to create a 2x2 matrix, filled by row
* **sign**(value) returns 1 when value is positive, -1 when negative, and 0 when tie 
* **strptime**(Date, format=curFormatOfDate) to transform time
    * weekdays(transformedTime)
    * $Hour
* **as.data.frame** to convert to data frame
* **make.names** to make syntactically valid names
* **factor** --- the same as OneHotEncoding in Python
    * choose most popular factor to be the reference, then create a feature for each of the other factors
    * As an example, consider the unordered factor variable "color", with levels "red", "green", and "blue". If "green" were the reference level, then we would add binary variables "colorred" and "colorblue" to a linear regression problem. All red examples would have colorred=1 and colorblue=0. All blue examples would have colorred=0 and colorblue=1. All green examples would have colorred=0 and colorblue=0.
    * R automatically sets the reference level to be the first alphabetically => to reset level use **reset**(data$variable, factorToBeTheReference)
* **grepl**("cat","dogs and cats",fixed=TRUE) returns true because "cat" exists in longer string
* **rowSums** to obtain counts for each row
* **ifelse**(TRUE,clauseIF,clauseELSE)
* **set.seed**(num) to get reproducible random numbers
* In order to read in an image as matrix, first read in as data frame, then convert to matrix, then to vector
* **colSums()** to sum across all columns
* **paste0**("x",list) prepend X to every element of list
* package **zoo** is used to manipulate time series data
    *Ex: lag(zoo(FluTrain$ILI), -2, na.pad=TRUE) creates a 2 week lag for every observation, which means to predict current week's value, use the observation from 2 weeks ago ; na.pad = TRUE adds missing values for the first two weeks of our dataset, where we can't compute the data from 2 weeks earlier.
* package **caTools** to split data into training and testing sets
    * sample.split(DependentVar, SplitRatio=percentageOfDataInTrainingSet)
        train = subset(data, split == TRUE)
          test = subset(data, split == FALSE)
* package **ROCR** to create a ROCR
    * prediction(predicted, actual)
    * performace(prediction_object, measure="auc" to get the auc value or measure = "tpr", x.measure = "fpr") to get true positive rate, and false positive rate)
    * plot (performance, colorize=TRUE)
    * as.numeric(perf@y.values)
    * plot(colorize = T/F, print.cutoffs.at=seq, text.adj)
* package **mice** to use Multiple Imputation by Chained Equations
    * complete(mice(dataSet)) to fill in missing values differently for each run
* package **rpart** and **rpart.plot** to build decision tree
    * model = rpart(Dependent ~ Independent, data= , method="class" for classification, minbucket=/cp=)
    * adds parms=list(loss=PenaltyMatrix) with self-defined Penalty Matrix to tell the algorithm to consider it when training the model
    * prp(model, digits=6 to get trailing digits) to plot tree
    * predict(model, newdata= , type = "class" is same as setting threshold > 0.5)
* package **randomForest**
    * nodesize = min number of observations in a subset
    * ntree = number of trees in a forest
    * varUsed(model, count=TRUE) to get the counts for each feature when it is selected for splitting
    * varImpPlot(model) to get the impurity measure for each feature
    * predict(type="prob" to get probabilities)
* package **caret** and **e1071** to perform cross validation using kfolds and a sequence of parameter values
    * Ex: cv with different choices of parameter cp for CART model
        numFolds = trainControl( method = "cv", number = 10 )
        cpGrid = expand.grid( .cp = seq(0.01,0.5,0.01)) 
        train(Reverse ~ Circuit + Issue, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
    * preProcess, then predict to get preprocessed data
* package **tm** to do bag of words
    * VCorpus(VectorSource(data$text)) to convert data into a corpus of texts
    * to select first piece of text, use double square brackets corpus[[1]]
    * tm_map (corpus, content_transformer(tolower)/removePunctuation/removeWords, c("words to remove"), stemDocument)
    * stopwords("english") to see all English stopwords
    * DocumentTermMatrix(corpus) to create bag of words matrix
        * inspect(matrix)
        * findFreqTerms(matrix, lowFreg=numAppearing)
        * removeSparseTerms(matrix, .97 to remove terms/features that don't appear at least 3% in corpus)
        * as.data.frame(as.matrix()) to convert list to dataframe
* package **ggplot** to do visualization
    * scatterplot = ggplot(dataSet, aes(x= var, y = var, color=NumericalVar/Factor))
        * + stat_smooth(method="lm", level=0.99 to get 99%CI shaded, color=)
        * + xlab="" + ylab="" 
    * scatterplot + geom_point(color=, size=, shape=17(triangle)) 
        * + geom_line(aes(group=1)?,linetype=2forDashed,alpha=0.5forLighterColor)
        * + ggtitle("name")
        * pdf("fileName.pdf"), print(plot), dev.off() to save plot to pdf
* package **SnowballC** 
* **as.factor**(variable) to convert variable to a factor variable, i.e. instead of viewing each instance as a value, view it as a category sort of

