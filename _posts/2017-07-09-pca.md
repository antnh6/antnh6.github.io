---
layout: post
title: PCA and ICA
tags: [data-analysis, unsupervised, preprocessing, data-transformation]
categories: notes
--- 
Below are different feature transformation algorithms.

## PCA (Principal Component Analysis) 
is a classic technique to reduce the number of dimensions we need to consider in a dataset (**dimensionality reduction**) and to reveal the **latent features** driving the patterns in our data. 

It does so by finding the directions(among all conceivable linear combinations of the current features) of maximum variance in our original high-dimensional data by minimizing the information loss when we project our data points onto these directions. This entails that PCA will transform our n-dimensional dataset into a *new* k-dimensional dataset that best "summarizes" our data[(source)][2]. These directions are called *principal components* and are actually *eigenvectors*, so they are orthogonal to one another. They can be also ranked in order of having the most variance to least. 

## Dimensionality Reduction
is useful in that it helps us:
* visualize our high-dimensional data in 2D/3D.
* reduce noise in our data (strongest principal components capturing the actual patterns in the data while smaller PCs representing noisy variations about those patterns).
* make other algorithms work better because fewer inputs (eigenfaces).

## Maximal Variance and Information Loss

* **Information Loss** is defined as the sum of all the projection lengths of our data onto a principle component, so by projecting each point onto the component with **maximal variance**, we will minimize the info loss the most. 

(It looks so in the diagram below, but I'm gonna have to read up on the math side of it).

<p align="center">
  <img src="../../img/post-img/unsupervised/pca/2.png" height="80%" width="80%">
</p>

## Interpretation of PCs
One thing to keep in mind is that the sign of a PCA dimension itself is not important, but the relative signs of features forming the PCA dimension are important.
https://onlinecourses.science.psu.edu/stat505/node/54


## How many PCs to use?
No fast track here. Best way to go is to train on different number of PCs, and see how accuracy responds -- cut off when it becomes apparent that adding more PCs is overfitting our data.
<br>
Tread lightly with performing feature selection BEFORE PCA though, because it might throw out crucial features that PCA would have been able to rescue otherwise.

## Limitations of PCA
* **Maximizing Spread**. The main assumption of PCA is that dimensions that reveal the largest spread among data points are the most useful. However, this may not be true. A popular counter example is the task of counting pancakes arranged in a stack when a stack is thin.
* **Orthogonal Components**. One major drawback of PCA is that the principal components it generates must be orthogonal. However, this assumption is restrictive as informative dimensions may not necessarily be orthogonal to each other.
* **Interpreting Components** Interpretations of generated components have to be inferred, and sometimes we may struggle to explain the combination of variables in a principal component.
[(source)][1]

## PCA is not Linear Regression
* On the left, we have the difference between the actual value and the predicted value (linReg). 
* On the right, it's the length of the projection (information loss). 
<p align="center">
  <img src="../../img/post-img/unsupervised/pca/1.png" height="80%" width="80%">
</p>

## Cool Visualization 
[here](http://setosa.io/ev/principal-component-analysis/)

###Biplot 

In the Cartesian Plane, a vector with coordinates perhaps (0.9, -0.1) would be one that heads 0.9 units positive in the x direction and 0.1 units negative in the y direction. We could also say that it overwhelmingly contributes to the x-variable and negligibly contributes to the y-variable. If the x-variable was "water" and the y-variable was "cream", would could say the vector "coffee" would have a much larger x value than y value.

Now, we can simply extrapolate this idea to what you're showing here. While we don't know a way to adequately define a name to "Dimension 1" and "Dimension 2", we know quite clearly what are the greatest contributors to each. Take a look at Detergents_Paper. We can roughly estimate that its coordinates would be (-5.2, -0.4). Hence we know it has a far greater contribution to "Dimension 1" than to "Dimension 2". Put all of the features together, and you get a really rough idea of what each "Dimension" means when considering the composition of those features projected onto the dimension.

non directional

ICA (Independent Component Analysis) 
directional
find parts of or edges or topics in docs
probability

RCA? (random)
generates random directions that somehow still captures enough info from original data for this to do quite well in practice 
tends to not project down to the lowest dimensional space possible
fast though

LDA (Linear Discreminant Analysis) finds a projection that discriminates based on the label
<p align="center">
  <img src="../../img/post-img/unsupervised/pca/4.png" height="50%" width="50%">
</p>


[1]: https://www.quora.com/What-is-an-intuitive-explanation-for-PCA/answer/Annalyn-Ng?srid=ugxJO
[2]: http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html